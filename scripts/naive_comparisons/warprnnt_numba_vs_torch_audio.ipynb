{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "warprnnt_numba_vs_torch_audio.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq4tJykSe_eE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e179f722-5261-4774-a827-1916d7d5c631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
            "Collecting numba\n",
            "  Downloading numba-0.55.0-1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n",
            "Collecting llvmlite<0.39,>=0.38.0rc1\n",
            "  Downloading llvmlite-0.38.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 9.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.22,>=1.18 in /usr/local/lib/python3.7/dist-packages (from numba) (1.19.5)\n",
            "Installing collected packages: llvmlite, numba\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "Successfully installed llvmlite-0.38.0 numba-0.55.0\n"
          ]
        }
      ],
      "source": [
        "# Update numba and restart\n",
        "\n",
        "# In a conda environment, you would use the following command\n",
        "# Update Numba to > 0.54\n",
        "# conda install -c numba numba\n",
        "# or\n",
        "# conda update -c numba numba\n",
        "\n",
        "# For pip based environments,\n",
        "# Update Numba to > 0.54\n",
        "import os\n",
        "import signal\n",
        "\n",
        "!pip install --upgrade numba\n",
        "\n",
        "# This will kill the kernel, click next cell to import the latest numba\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numba\n",
        "print(numba.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84lf3plWf6c-",
        "outputId": "cd10d220-3c4c-4382-b924-577fec97f76b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.55.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/titu1994/warprnnt_numba.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmgxiF1vfqa0",
        "outputId": "7b4ab8c3-c2d9-40c1-cce2-01582f4a4e1e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/titu1994/warprnnt_numba.git\n",
            "  Cloning https://github.com/titu1994/warprnnt_numba.git to /tmp/pip-req-build-zqovshoc\n",
            "  Running command git clone -q https://github.com/titu1994/warprnnt_numba.git /tmp/pip-req-build-zqovshoc\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from warprnnt-numba==0.1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from warprnnt-numba==0.1.0) (0.55.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->warprnnt-numba==0.1.0) (57.4.0)\n",
            "Requirement already satisfied: numpy<1.22,>=1.18 in /usr/local/lib/python3.7/dist-packages (from numba->warprnnt-numba==0.1.0) (1.19.5)\n",
            "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /usr/local/lib/python3.7/dist-packages (from numba->warprnnt-numba==0.1.0) (0.38.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->warprnnt-numba==0.1.0) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import os\n",
        "\n",
        "print(\"Torch :\", torch.__version__)\n",
        "print(\"Torch Audio:\", torchaudio.__version__)\n",
        "print(\"Torch audio version must be >= 0.10.0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdvtukmUgrLA",
        "outputId": "574f6e35-7092-4893-9630-eac6e05e9be7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch : 1.10.0+cu111\n",
            "Torch Audio: 0.10.0+cu111\n",
            "Torch audio version must be >= 0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warprnnt_numba\n",
        "print(warprnnt_numba.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgGYHW9qfkAv",
        "outputId": "2e43818a-2279-42f1-e46d-7c72b238e0c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numba\n",
        "warprnnt_numba.numba_utils.numba_cuda_is_supported(numba.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtPbeoKalFNo",
        "outputId": "2cf76f55-c051-4a14-a9b9-f2af381a67ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Vdfa2ynVlMd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import subprocess\n",
        "import traceback\n",
        "\n",
        "import torch\n",
        "import torch.utils.benchmark as benchmark\n",
        "\n",
        "from torchaudio.transforms import RNNTLoss\n",
        "from warprnnt_numba.rnnt_loss import RNNTLossNumba\n",
        "\n",
        "\n",
        "DEVICE = 'cuda'"
      ],
      "metadata": {
        "id": "msYY_7CejIUX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_gen(bs, t=200, u=100, v=1024, dtype=torch.float32):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    shape = [bs, t, u, v + 1]\n",
        "    torch.manual_seed(0)\n",
        "    x = torch.randn(*shape, dtype=dtype, device=DEVICE)\n",
        "    x_len = torch.randint(t, size=[bs], device=x.device, dtype=torch.int32)\n",
        "    y = torch.randint(v, size=[bs, u - 1], device=x.device, dtype=torch.int32)\n",
        "    y_len = torch.randint(u, size=[bs], device=x.device, dtype=torch.int32)\n",
        "\n",
        "    # enforce some RNNT input constraints\n",
        "    rand_idx = torch.randint(bs, size=[1])\n",
        "    x_len[rand_idx] = t\n",
        "    y_len[rand_idx] = u - 1\n",
        "\n",
        "    return x, x_len, y, y_len\n",
        "\n",
        "\n",
        "def check_time_pt(x, x_len, y, y_len, fastemit_lambda=None):\n",
        "    blank = x.shape[-1] - 1\n",
        "    rnnt_loss = RNNTLoss(blank=blank, clamp=-1., reduction=\"none\")\n",
        "\n",
        "    try:\n",
        "        _ = rnnt_loss(logits=x, targets=y, logit_lengths=x_len, target_lengths=y_len)\n",
        "    except NotImplementedError:\n",
        "        print()\n",
        "        print(\"RNNT Loss not available on this platform. Could not compute Pytorch Audio RNNT Loss.\")\n",
        "        print(\"Original error below :\")\n",
        "        print(traceback.format_exc())\n",
        "        exit(1)\n",
        "\n",
        "\n",
        "def check_time_cuda(x, x_len, y, y_len, fastemit_lambda=0.0):\n",
        "    blank = x.shape[-1] - 1\n",
        "    rnnt_loss = RNNTLossNumba(blank=blank, reduction='none', fastemit_lambda=fastemit_lambda)\n",
        "\n",
        "    # Numba doesnt support fp16\n",
        "    if x.dtype != torch.float32:\n",
        "        x = x.float()\n",
        "\n",
        "    _ = rnnt_loss(acts=x, labels=y, act_lens=x_len, label_lens=y_len)"
      ],
      "metadata": {
        "id": "TnhYfzIolEFV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print CUDA environment\n",
        "results = []\n",
        "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, encoding='utf-8')\n",
        "print(result.stdout)\n",
        "result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True, encoding='utf-8')\n",
        "print(result.stdout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR6N8mNC3aNI",
        "outputId": "89d8615d-327e-4fb7-91e9-f2bfa9845645"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 24 07:03:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    56W / 149W |   8207MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            "GPU 0: Tesla K80 (UUID: GPU-1b23b0b5-82e6-d973-44f2-e891f4f06c48)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "torch.cuda.empty_cache()\n",
        "print(\"GPU Memory :\", torch.cuda.memory_summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIoG4_mq5Uw7",
        "outputId": "1f6036ce-735d-4271-86ba-8e1e7b739dd6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory : |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  160158 KB |    7668 MB |    1513 GB |    1513 GB |\n",
            "|       from large pool |  160156 KB |    7668 MB |    1497 GB |    1497 GB |\n",
            "|       from small pool |       1 KB |       0 MB |      16 GB |      16 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  160158 KB |    7668 MB |    1513 GB |    1513 GB |\n",
            "|       from large pool |  160156 KB |    7668 MB |    1497 GB |    1497 GB |\n",
            "|       from small pool |       1 KB |       0 MB |      16 GB |      16 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  163840 KB |    7690 MB |  214512 MB |  214352 MB |\n",
            "|       from large pool |  161792 KB |    7688 MB |  214498 MB |  214340 MB |\n",
            "|       from small pool |    2048 KB |       2 MB |      14 MB |      12 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3682 KB |   22348 KB |    1788 GB |    1788 GB |\n",
            "|       from large pool |    1635 KB |   20324 KB |    1772 GB |    1772 GB |\n",
            "|       from small pool |    2046 KB |    2046 KB |      16 GB |      16 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       4    |      18    |  311752    |  311748    |\n",
            "|       from large pool |       1    |       4    |   30048    |   30047    |\n",
            "|       from small pool |       3    |      14    |  281704    |  281701    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       4    |      18    |  311752    |  311748    |\n",
            "|       from large pool |       1    |       4    |   30048    |   30047    |\n",
            "|       from small pool |       3    |      14    |  281704    |  281701    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       2    |       5    |     523    |     521    |\n",
            "|       from large pool |       1    |       4    |     516    |     515    |\n",
            "|       from small pool |       1    |       1    |       7    |       6    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       3    |       8    |  103788    |  103785    |\n",
            "|       from large pool |       1    |       5    |   14539    |   14538    |\n",
            "|       from small pool |       2    |       4    |   89249    |   89247    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basedir = f\"results/numba_vs_torch_audio/\"\n",
        "if not os.path.exists(basedir):\n",
        "    os.makedirs(basedir, exist_ok=True)"
      ],
      "metadata": {
        "id": "YDpe6z_e6TCB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare takes a list of measurements which we'll save in results.\n",
        "results = []\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "results_path = os.path.join(basedir, 'rnnt_results.pkl')\n",
        "\n",
        "for b in [1, 4, 8]:  # 1, 4, 8, 16, 32, 64 (on 32 GB GPUs)\n",
        "    for t in [200, 400]:  # 200, 400, 600 (LibriSpeech with 4x and 8x stride, on 32 GB GPUs)\n",
        "        for u in [100, 200]:  # 100, 200  # (char enc, subword enc)\n",
        "            for v in [28, 1024]:  # 28, 1024  # (char encoding, Conformer RNNT Vocab Size)\n",
        "                for fastemit_lambda in [0.0, 0.001]:  # 0.0, 0.001  # (Google FastEmit regularization, no extra memory)\n",
        "                    for dtype in [torch.float32]:  # (AMP / FP32; Note: Numba impl will force cast to fp32)\n",
        "\n",
        "                        # label and sub_label are the rows\n",
        "                        # description is the column\n",
        "                        label = 'RNNTLoss'\n",
        "                        sub_label = (\n",
        "                            f'[b={b}, t={t}, u={u}, v={v}, '\n",
        "                            f'fastemit_lambda={fastemit_lambda}, '\n",
        "                            f'dtype={dtype}]'\n",
        "                        )\n",
        "\n",
        "                        print(\"Computing :\", sub_label)\n",
        "\n",
        "                        # Pytorch\n",
        "                        env = 'TorchAudio'\n",
        "                        x, x_len, y, y_len = data_gen(b, t, u, v, dtype=dtype)\n",
        "\n",
        "                        if fastemit_lambda == 0.0:\n",
        "                            # weird case of cuda illegal mem access beyond this config for fp 16 / fp 32 for batchsize=32\n",
        "                            # works uptil b=32, t=329, u=200, v=1024 then fails above that for fp16\n",
        "                            # Also, setup b=32, t=600, u=100, v=1024 and above fails for fp32\n",
        "                            if (b * t * u * v) < (2 ** 31):\n",
        "                                # fmt: off\n",
        "                                t0 = benchmark.Timer(\n",
        "                                    stmt='check_time_pt(x, x_len, y, y_len, fastemit_lambda)',\n",
        "                                    setup=\"from __main__ import check_time_pt\",\n",
        "                                    globals={'x': x, 'x_len': x_len, 'y': y, 'y_len': y_len,\n",
        "                                              'fastemit_lambda': fastemit_lambda},\n",
        "                                    label=label,\n",
        "                                    sub_label=sub_label,\n",
        "                                    description=env,\n",
        "                                    num_threads=32\n",
        "                                ).blocked_autorange(min_run_time=1.0)\n",
        "                                # fmt: on\n",
        "\n",
        "                                results.append(t0)\n",
        "\n",
        "                        del x, x_len, y_len\n",
        "\n",
        "                        # Numba\n",
        "                        env = 'Numba'\n",
        "                        x, x_len, y, y_len = data_gen(b, t, u, v, dtype=dtype)\n",
        "\n",
        "                        # fmt: off\n",
        "                        if b <= 16:\n",
        "                          t0 = benchmark.Timer(\n",
        "                              stmt='check_time_cuda(x, x_len, y, y_len, fastemit_lambda)',\n",
        "                              setup=\"from __main__ import check_time_cuda\",\n",
        "                              globals={'x': x, 'x_len': x_len, 'y': y, 'y_len': y_len,\n",
        "                                        'fastemit_lambda': fastemit_lambda},\n",
        "                              label=label,\n",
        "                              sub_label=sub_label,\n",
        "                              description=env,\n",
        "                              num_threads=32\n",
        "                          ).blocked_autorange(min_run_time=1.0)\n",
        "                          # fmt: on\n",
        "\n",
        "                          results.append(t0)\n",
        "                        \n",
        "                        del x, x_len, y_len\n",
        "\n",
        "with open(results_path, 'wb') as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "with open(results_path, 'rb') as f:\n",
        "    results = pickle.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPDUgiZhlTEJ",
        "outputId": "706a6fcd-552c-4c1b-89c3-57b57aa4e185"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing : [b=1, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:724: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (26) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing : [b=1, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:724: NumbaPerformanceWarning: Grid size (4) < 2 * SM count (26) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing : [b=4, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:724: NumbaPerformanceWarning: Grid size (8) < 2 * SM count (26) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing : [b=8, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print()\n",
        "\n",
        "compare = benchmark.Compare(results)\n",
        "compare.colorize()\n",
        "compare.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y80iSqdMlV1s",
        "outputId": "142505da-4d01-42f1-dfd3-36f0b63f8473"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[--------------------------------------------- RNNTLoss ---------------------------------------------]\n",
            "                                                                               |  TorchAudio  |  Numba\n",
            "32 threads: ------------------------------------------------------------------------------------------\n",
            "      [b=1, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[92m\u001b[1m    3.8   \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m  7.5\u001b[0m\u001b[0m\n",
            "      [b=1, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |    8.1\n",
            "      [b=1, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m   21.7   \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  7.3\u001b[0m\u001b[0m\n",
            "      [b=1, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |  \u001b[92m\u001b[1m  7.3\u001b[0m\u001b[0m\n",
            "      [b=1, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |      7.3     |    8.0\n",
            "      [b=1, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |    8.1\n",
            "      [b=1, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m   59.3   \u001b[0m\u001b[0m  |    9.6\n",
            "      [b=1, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |    9.6\n",
            "      [b=1, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |      6.3     |    8.5\n",
            "      [b=1, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |    8.6\n",
            "      [b=1, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m   80.2   \u001b[0m\u001b[0m  |   10.1\n",
            "      [b=1, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |   10.0\n",
            "      [b=1, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[2m\u001b[91m   12.4   \u001b[0m\u001b[0m  |    9.5\n",
            "      [b=1, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |    9.6\n",
            "      [b=1, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m  166.0   \u001b[0m\u001b[0m  |   14.4\n",
            "      [b=1, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |   14.4\n",
            "      [b=4, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[2m\u001b[91m    8.8   \u001b[0m\u001b[0m  |   12.7\n",
            "      [b=4, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |   13.8\n",
            "      [b=4, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m   47.9   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 17.6\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |  \u001b[2m\u001b[91m 17.8\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[2m\u001b[91m   16.3   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 15.2\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |  \u001b[2m\u001b[91m 15.2\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m  121.4   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 25.8\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |  \u001b[2m\u001b[91m 26.1\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[2m\u001b[91m   15.5   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 15.4\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |  \u001b[2m\u001b[91m 15.9\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m  126.1   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 26.6\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |  \u001b[2m\u001b[91m 26.1\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[31m\u001b[1m   27.3   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 20.2\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |  \u001b[2m\u001b[91m 20.4\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m  303.4   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 42.0\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |  \u001b[31m\u001b[1m 41.6\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[2m\u001b[91m   15.9   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 20.8\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |  \u001b[2m\u001b[91m 21.2\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m   89.7   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 32.5\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |  \u001b[2m\u001b[91m 32.3\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[31m\u001b[1m   34.1   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 25.8\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |  \u001b[2m\u001b[91m 26.5\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m  199.5   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 47.2\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |  \u001b[31m\u001b[1m 48.3\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[31m\u001b[1m   27.1   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 26.6\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |  \u001b[2m\u001b[91m 26.1\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m  235.3   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 48.9\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |  \u001b[31m\u001b[1m 48.0\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[31m\u001b[1m   51.6   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 34.6\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |              |  \u001b[2m\u001b[91m 34.9\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m  330.4   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 78.8\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |              |  \u001b[31m\u001b[1m 78.0\u001b[0m\u001b[0m\n",
            "\n",
            "Times are in milliseconds (ms).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8KdpWgGhmP8K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}