{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "warprnnt_numba_vs_speechbrain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq4tJykSe_eE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3776aa0-86f3-478b-d0a5-0d9115797b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
            "Collecting numba\n",
            "  Downloading numba-0.55.1-1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 30.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n",
            "Collecting llvmlite<0.39,>=0.38.0rc1\n",
            "  Downloading llvmlite-0.38.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 9.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.22,>=1.18 in /usr/local/lib/python3.7/dist-packages (from numba) (1.19.5)\n",
            "Installing collected packages: llvmlite, numba\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "Successfully installed llvmlite-0.38.0 numba-0.55.1\n"
          ]
        }
      ],
      "source": [
        "# Update numba and restart\n",
        "\n",
        "# In a conda environment, you would use the following command\n",
        "# Update Numba to > 0.54\n",
        "# conda install -c conda-forge numba>=0.54\n",
        "# or\n",
        "# conda update -c conda-forge numba>=0.54\n",
        "\n",
        "# For pip based environments,\n",
        "# Update Numba to > 0.54\n",
        "import os\n",
        "import signal\n",
        "\n",
        "!pip install --upgrade numba\n",
        "\n",
        "# This will kill the kernel, click next cell to import the latest numba\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive comparison between WarpRNNT Numba and SpeechBrain RNNT [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/titu1994/warprnnt_numba/blob/master/scripts/naive_comparisons/warprnnt_numba_vs_speechbrain.ipynb)\n",
        "\n",
        "This notebook is a colab compatible way to do a **naive comparison** between the two loss functions. \n",
        "\n",
        "*Therefore no conclusions can be reached from this notebook, both are useful in many contexts.*\n",
        "\n",
        "-----\n",
        "\n",
        "Note that due to some dangling reference issue with running PyTorch `benchmark.Timer` with global variables for the inputs to the function, we will be writing the code in the notebook and in parallel exporting  the code snippets into a new file called `script.py` which will then be executed to write out the results.\n",
        "\n",
        "-----\n",
        "\n",
        "## THIS NOTEBOOK MUST BE RUN TOP TO BOTTOM ONLY. \n",
        "\n",
        "-----\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EHaf-XbRrZws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that a recent Numba version has been installed. Anything > 0.53 will do."
      ],
      "metadata": {
        "id": "83b1lg7Tsd8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numba\n",
        "print(numba.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84lf3plWf6c-",
        "outputId": "7dd0b076-1171-4c0a-a6cd-a1512531c4d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.55.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the `warprnnt_numba` library from https://github.com/titu1994/warprnnt_numba.git"
      ],
      "metadata": {
        "id": "3h6YpSg9so89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/titu1994/warprnnt_numba.git"
      ],
      "metadata": {
        "id": "RmgxiF1vfqa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install `speechbrain`"
      ],
      "metadata": {
        "id": "dpOn3sHsRcjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "Z5gGJLgyRbk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speechbrain"
      ],
      "metadata": {
        "id": "9hPGJd50RnxL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility IPython Magic functions\n",
        "\n",
        "The following two functions are to denote cells that export their code content into `scripts.py`. \n",
        "\n",
        "-----\n",
        "\n",
        "## NOTE\n",
        "\n",
        "Rerunning a cell multiple times will duplicate the code inside the script, so only run this notebook top to bottom and return here to run again !"
      ],
      "metadata": {
        "id": "7UylHQf4svkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.magic import register_cell_magic\n",
        "\n",
        "@register_cell_magic\n",
        "def exec_write_cell(line, cell):\n",
        "    # Run and save python code block to a file\n",
        "\n",
        "    with open(line, 'a', encoding='utf8') as pyf:\n",
        "        pyf.write(cell)\n",
        "        pyf.write(\"\\n\\n\")\n",
        "\n",
        "    code = compile(cell, line, 'exec')\n",
        "    exec(code, globals())\n",
        "    print(\"---> wrote cells to file :\", line)\n",
        "\n",
        "@register_cell_magic\n",
        "def write_cell(line, cell):\n",
        "    # Save python code block to a file, but do not run it.\n",
        "\n",
        "    with open(line, 'a', encoding='utf8') as pyf:\n",
        "        pyf.write(cell)\n",
        "        pyf.write(\"\\n\\n\")\n",
        "\n",
        "    print(\"---> wrote cells to file (and did not execute):\", line)"
      ],
      "metadata": {
        "id": "7SffNvdEWbWG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Be sure to delete the file cells if creating a new \n",
        "if os.path.exists('script.py'):\n",
        "  os.remove('script.py')"
      ],
      "metadata": {
        "id": "neo_P7eUYbg-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%exec_write_cell script.py\n",
        "\n",
        "import torch\n",
        "import speechbrain\n",
        "import os\n",
        "\n",
        "print(\"Torch :\", torch.__version__)\n",
        "print(\"Speechbrain:\", speechbrain.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdvtukmUgrLA",
        "outputId": "6c694ad4-8b71-492a-9d54-d0383f35aa2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch : 1.10.0+cu111\n",
            "Speechbrain: 0.5.11\n",
            "---> wrote cells to file : script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%exec_write_cell script.py\n",
        "\n",
        "import warprnnt_numba\n",
        "print(\"warprnnt_numba:\", warprnnt_numba.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgGYHW9qfkAv",
        "outputId": "3f3776eb-0b92-447f-b243-a0c2d6eb38d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "warprnnt_numba: 0.4.0\n",
            "---> wrote cells to file : script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%exec_write_cell script.py\n",
        "\n",
        "import numba\n",
        "cuda_supported = warprnnt_numba.numba_utils.numba_cuda_is_supported(numba.__version__)\n",
        "print(\"Numba supports CUDA:\", cuda_supported)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtPbeoKalFNo",
        "outputId": "db0eba9e-1efc-478a-c13a-3d1151feecc4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numba supports CUDA: True\n",
            "---> wrote cells to file : script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper methods \n",
        "\n",
        "Below are some helper methods that will build the loss functions and then call them on some random data. \n",
        "\n",
        "-----\n",
        "\n",
        "Note that for a given set of input arguments to `data_gen()`, the seeds are set in such a way that if the shape matches (via variable `bs`, `t`, `u` and `v`) then the same tensor is generated. This is for fair comparison between same inputs for two different loss functions. "
      ],
      "metadata": {
        "id": "JxAyYxYz1-3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%exec_write_cell script.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import subprocess\n",
        "import traceback\n",
        "\n",
        "import torch\n",
        "import torch.utils.benchmark as benchmark\n",
        "\n",
        "from speechbrain.nnet.loss.transducer_loss import TransducerLoss\n",
        "from warprnnt_numba.rnnt_loss import RNNTLossNumba\n",
        "\n",
        "\n",
        "DEVICE = 'cuda'"
      ],
      "metadata": {
        "id": "msYY_7CejIUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa82a70-e5fd-42d9-9efa-0af893828a9f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---> wrote cells to file : script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Losses\n",
        "\n",
        "For comparison, we use <br>\n",
        "1) SpeechBrain RNNT Loss <br>\n",
        "2) Numba WarpRNNT Loss <br>\n",
        "\n",
        "-----\n",
        "\n",
        "Differences between the two losses - \n",
        "* SpeechBrain does not currently support [FastEmit Regulization](https://arxiv.org/abs/2010.11148). For such cases we skip the calculation of the loss and leave the result a blank row.\n",
        "\n",
        "* Numba does not currently support float16 CUDA calls, therefore we will test only fp32. If a fp16 tensor is passed to Numba loss, it will explicitly upcast it fp32 before computing the loss (at the cost of 2x memory for the input tensor). \n",
        "\n",
        "* Sometimes at Batch size 32 and large dimensions of T, U and V, SpeechBrain RNNT loss will OOM, so we explicitly check for input size in bytes and skip those scenarios where activation tensor > 4 GB of memory when gradients need to be calculated.\n"
      ],
      "metadata": {
        "id": "B5XTtQCRuHkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%exec_write_cell script.py\n",
        "\n",
        "global x, x_len, y, y_len\n",
        "\n",
        "def data_gen(bs, t=200, u=100, v=1024, dtype=torch.float32):\n",
        "    global x, x_len, y, y_len\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    shape = [bs, t, u, v + 1]\n",
        "    torch.manual_seed(0)\n",
        "    x = torch.randn(*shape, dtype=dtype, device=DEVICE, requires_grad=False)\n",
        "    x_len = torch.randint(t, size=[bs], device=DEVICE, dtype=torch.int32)\n",
        "    y = torch.randint(v, size=[bs, u - 1], device=DEVICE, dtype=torch.int32)\n",
        "    y_len = torch.randint(u, size=[bs], device=DEVICE, dtype=torch.int32)\n",
        "\n",
        "    # enforce some RNNT input constraints\n",
        "    rand_idx = torch.randint(bs, size=[1])\n",
        "    x_len[rand_idx] = t\n",
        "    y_len[rand_idx] = u - 1\n",
        "\n",
        "    return x, x_len, y, y_len\n",
        "\n",
        "\n",
        "def check_time_sb(x, x_len, y, y_len, fastemit_lambda=None):\n",
        "    blank = x.shape[-1] - 1\n",
        "    rnnt_loss = TransducerLoss(blank=blank, reduction=\"none\")\n",
        "\n",
        "    try:\n",
        "        # need to explicitly apply log_softmax\n",
        "        x = torch.log_softmax(x, axis=-1) \n",
        "        _ = rnnt_loss(log_probs=x, labels=y, T=x_len, U=y_len)\n",
        "    except NotImplementedError:\n",
        "        print()\n",
        "        print(\"RNNT Loss not available on this platform. Could not compute SpeechBrain RNNT Loss.\")\n",
        "        print(\"Original error below :\")\n",
        "        print(traceback.format_exc())\n",
        "        exit(1)\n",
        "\n",
        "\n",
        "def check_time_numba(x, x_len, y, y_len, fastemit_lambda=0.0, clamp=-1.0):\n",
        "    blank = x.shape[-1] - 1\n",
        "    rnnt_loss = RNNTLossNumba(blank=blank, reduction='none', fastemit_lambda=fastemit_lambda, clamp=clamp)\n",
        "\n",
        "    # Numba doesnt support fp16\n",
        "    if x.dtype != torch.float32:\n",
        "        x = x.float()\n",
        "\n",
        "    _ = rnnt_loss(acts=x, labels=y, act_lens=x_len, label_lens=y_len)\n",
        "\n",
        "\n",
        "def load_results(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    results = pickle.load(f)\n",
        "    return results\n",
        "\n",
        "def save_results(results, path):\n",
        "  with open(path, 'wb') as f:\n",
        "    pickle.dump(results, f)"
      ],
      "metadata": {
        "id": "TnhYfzIolEFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f4dace-d21d-4a4f-e02f-2d92cd740758"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---> wrote cells to file : script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Info\n",
        "\n",
        "The script should emit some key info such as which GPU is being used, how much memory it has and how much is free/allocated at the moment."
      ],
      "metadata": {
        "id": "8WrQLJnLwKGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%exec_write_cell script.py\n",
        "\n",
        "# Print CUDA environment\n",
        "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, encoding='utf-8')\n",
        "print(result.stdout)\n",
        "result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True, encoding='utf-8')\n",
        "print(result.stdout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR6N8mNC3aNI",
        "outputId": "28a849ea-c141-4364-a790-33ab39cec76d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb  1 08:27:54 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    27W /  70W |    104MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            "GPU 0: Tesla T4 (UUID: GPU-813da1c7-e3b9-50ad-128d-14ead9928af8)\n",
            "\n",
            "---> wrote cells to file : script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%exec_write_cell script.py\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "print(\"GPU Memory :\", torch.cuda.memory_summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIoG4_mq5Uw7",
        "outputId": "abd0c720-7370-421f-a307-52b829dd08d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory : |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "---> wrote cells to file : script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%exec_write_cell script.py\n",
        "\n",
        "basedir = f\"results/warprnnt_vs_speechbrain/\"\n",
        "if not os.path.exists(basedir):\n",
        "    os.makedirs(basedir, exist_ok=True)"
      ],
      "metadata": {
        "id": "YDpe6z_e6TCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea3ce54-8f1a-402c-8e5a-e12426244ddf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---> wrote cells to file : script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core script\n",
        "\n",
        "This cell is the main portion of the notebook, which normally should execute with certain combinations noted below.\n",
        "\n",
        "However, during testing it seems memory is not properly released inside the loop even with explicit None cast and global variables to prevent duplicate referrences. So this snippet simply writes out to the script instead of executing itself.\n",
        "\n",
        "-----\n",
        "\n",
        "### Permutations\n",
        "\n",
        "The ranges have been selected for general Librispeech training. \n",
        "\n",
        "Batch size depends on the variable `REQUIRES_GRAD`. Since gradient shape is same as shape of the input joint, it requires roughly 2x the memory so batch size must be halved. \n",
        "\n",
        "* `b`: [1, 4, 8, 16]. [32] is added if loss is being computed for inference only. 32 GB memory can go upto 64 with Numba for inference and 32 for training.\n",
        "* `t`: [200, 400]. Average length of LS is 16 seconds, with 4x stride ~ 400 timesteps, and with 8x stride of encoder its ~ 200 timesteps.\n",
        "* `u`: [100, 200]. Depends on how the text was encoded - character encoding (upto 400+ characters) to subword encoding (100-200 sub-words). \n",
        "* `v`: [28, 1024]. Represents vocabulary size of the model. 28 is for character encoding - 26 lower case alphabet, space and apostrophe. 1024 is for sub-word encoding with fixed vocabulary size - Google papers tend towards 1024 for their RNNT models (though some are upto 4096).\n",
        "* `fastemit_lambda`: [0.0, 0.001]. FastEmit regularization strength. 0.0 means it is disabled, and any value > 0 will perform fastemit regularization for numba loss. Skipped for SpeechBrain loss.\n",
        "* `dtype`: [torch.float32]. Fixed to float32 for now, since we cant do the largest test suite due to memory constraints. Will be removed once numba supports float16 for CUDA.\n",
        "* `clamp`: [-1, 0.1]. Factor for gradient clamping. If -1, it is disabled and any value > 0 will enable the gradient clamping step in numba. Skipped for now as SpeechBrain does not support it."
      ],
      "metadata": {
        "id": "OH3xcfBewVFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%exec_write_cell script.py\n",
        "\n",
        "REQUIRES_GRAD = True\n",
        "\n",
        "print(\"Gradients will be computed :\", REQUIRES_GRAD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSn9sOKUv_UI",
        "outputId": "37e8dbf4-09bc-4c5f-c7cb-c0e3158d37f4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients will be computed : True\n",
            "---> wrote cells to file : script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%write_cell script.py\n",
        "\n",
        "# Compare takes a list of measurements which we'll save in results.\n",
        "global results\n",
        "results = []\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "results_path = os.path.join(basedir, 'rnnt_results.pkl')\n",
        "save_results(results, results_path)\n",
        "del results\n",
        "\n",
        "\n",
        "batchsizes = [1, 4, 8, 16]\n",
        "\n",
        "if not REQUIRES_GRAD:\n",
        "  batchsizes.append(32)\n",
        "\n",
        "for b in batchsizes:  # 1, 4, 8, 16, 32, 64 (on 32 GB GPUs)\n",
        "    for t in [200, 400]:  # 200, 400, 600 (LibriSpeech with 4x and 8x stride, on 32 GB GPUs)\n",
        "        for u in [100, 200]:  # 100, 200  # (char enc, subword enc)\n",
        "            for v in [28, 1024,]:  # 28, 1024  # (char encoding, Conformer RNNT Vocab Size)\n",
        "                for fastemit_lambda in [0.0, 0.001]:  # 0.0, 0.001  # (Google FastEmit regularization, no extra memory)\n",
        "                    for dtype in [torch.float32]:  # (AMP / FP32; Note: Numba impl will force cast to fp32)\n",
        "                        global x, x_len, y, y_len\n",
        "                        x = None\n",
        "                        x_len = None\n",
        "                        y = None\n",
        "                        y_len = None\n",
        "\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                        # label and sub_label are the rows\n",
        "                        # description is the column\n",
        "                        label = 'RNNTLoss'\n",
        "                        sub_label = (\n",
        "                            f'[b={b}, t={t}, u={u}, v={v}, '\n",
        "                            f'fastemit_lambda={fastemit_lambda}, '\n",
        "                            f'dtype={dtype}]'\n",
        "                        )\n",
        "\n",
        "                        print(\"Computing :\", sub_label)\n",
        "\n",
        "                        # SpeechBrain\n",
        "                        env = 'SpeechBrain'\n",
        "\n",
        "                        if fastemit_lambda == 0.0:\n",
        "                            x, x_len, y, y_len = data_gen(b, t, u, v, dtype=dtype)\n",
        "\n",
        "                            if REQUIRES_GRAD:\n",
        "                              x.requires_grad = True\n",
        "\n",
        "                            # SpeechBrain : Runs out of memory at [16, 400, 200, 600+] \n",
        "                            # so skip anything above 4 GB of memory for input to joint at fp32 (4 bytes)\n",
        "                            if (b * t * u * v) * 4 < (2 ** 32):\n",
        "                              # fmt: off\n",
        "                              t0 = benchmark.Timer(\n",
        "                                  stmt='check_time_sb(x, x_len, y, y_len, fastemit_lambda)',\n",
        "                                  setup=\"from __main__ import check_time_sb;\",\n",
        "                                  globals={'x': x, 'x_len': x_len, 'y': y, 'y_len': y_len,\n",
        "                                            'fastemit_lambda': fastemit_lambda,},\n",
        "                                  label=label,\n",
        "                                  sub_label=sub_label,\n",
        "                                  description=env,\n",
        "                                  num_threads=torch.get_num_threads(),\n",
        "                              ).blocked_autorange(min_run_time=1.0)\n",
        "                              # fmt: on\n",
        "\n",
        "                              results = load_results(results_path)\n",
        "                              results.append(t0)\n",
        "                              save_results(results, results_path)\n",
        "                              del results, t0\n",
        "                                \n",
        "                            del x, x_len, y_len\n",
        "                            \n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                        # Numba\n",
        "                        env = 'Numba'\n",
        "                        x, x_len, y, y_len = data_gen(b, t, u, v, dtype=dtype)\n",
        "\n",
        "                        if REQUIRES_GRAD:\n",
        "                              x.requires_grad = True\n",
        "\n",
        "                        # fmt: off\n",
        "                        t0 = benchmark.Timer(\n",
        "                            stmt='check_time_numba(x, x_len, y, y_len, fastemit_lambda);',\n",
        "                            setup=\"from __main__ import check_time_numba;\",\n",
        "                            globals={'x': x, 'x_len': x_len, 'y': y, 'y_len': y_len,\n",
        "                                      'fastemit_lambda': fastemit_lambda,},\n",
        "                            label=label,\n",
        "                            sub_label=sub_label,\n",
        "                            description=env,\n",
        "                            num_threads=torch.get_num_threads(),\n",
        "                        ).blocked_autorange(min_run_time=1.0)\n",
        "                        # fmt: on\n",
        "\n",
        "                        results = load_results(results_path)\n",
        "                        results.append(t0)\n",
        "                        save_results(results, results_path)\n",
        "                        del results, t0\n",
        "\n",
        "                        del x, x_len, y, y_len\n",
        "                        torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPDUgiZhlTEJ",
        "outputId": "72e011d9-a3c1-4681-8e9c-43c84f454f33"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---> wrote cells to file (and did not execute): script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute script\n",
        "\n",
        "Now that the script has the code contents necessary to perform the evaluations, execute it from the shell"
      ],
      "metadata": {
        "id": "FctYHrZLzV1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training mode"
      ],
      "metadata": {
        "id": "-3BS1JaVy8v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python script.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xqum-zROWpLC",
        "outputId": "b6ebc5eb-13f8-4d82-e0ca-95a2112d7ad8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch : 1.10.0+cu111\n",
            "Speechbrain: 0.5.11\n",
            "warprnnt_numba: 0.4.0\n",
            "Numba supports CUDA: True\n",
            "Tue Feb  1 08:29:15 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    27W /  70W |   1423MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            "GPU 0: Tesla T4 (UUID: GPU-813da1c7-e3b9-50ad-128d-14ead9928af8)\n",
            "\n",
            "GPU Memory : |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "Gradients will be computed : True\n",
            "Computing : [b=1, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Computing : [b=1, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (4) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Computing : [b=4, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (8) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Computing : [b=8, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (16) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Computing : [b=16, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print out results\n",
        "\n",
        "Since the output has been written to a pickle file, print out the output of the script above."
      ],
      "metadata": {
        "id": "aOtw0lHxz2lO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print()\n",
        "\n",
        "results_path = os.path.join(basedir, 'rnnt_results.pkl')\n",
        "results = load_results(results_path)\n",
        "compare = benchmark.Compare(results)\n",
        "compare.colorize()\n",
        "compare.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y80iSqdMlV1s",
        "outputId": "c40164c7-b677-4cdb-efae-fa22bfc4d861"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[---------------------------------------------- RNNTLoss ----------------------------------------------]\n",
            "                                                                                |  SpeechBrain  |  Numba\n",
            "1 threads: ---------------------------------------------------------------------------------------------\n",
            "      [b=1, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       2.6     |    6.7\n",
            "      [b=1, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |  \u001b[34m\u001b[1m  6.0\u001b[0m\u001b[0m\n",
            "      [b=1, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |       2.8     |    8.7\n",
            "      [b=1, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |    8.2\n",
            "      [b=1, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |  \u001b[34m\u001b[1m     2.6   \u001b[0m\u001b[0m  |    6.2\n",
            "      [b=1, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    6.2\n",
            "      [b=1, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |       3.6     |  \u001b[2m\u001b[91m 11.3\u001b[0m\u001b[0m\n",
            "      [b=1, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m 11.4\u001b[0m\u001b[0m\n",
            "      [b=1, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |  \u001b[34m\u001b[1m     2.5   \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m  5.7\u001b[0m\u001b[0m\n",
            "      [b=1, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |  \u001b[92m\u001b[1m  5.5\u001b[0m\u001b[0m\n",
            "      [b=1, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |       3.8     |  \u001b[2m\u001b[91m 11.3\u001b[0m\u001b[0m\n",
            "      [b=1, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m 11.1\u001b[0m\u001b[0m\n",
            "      [b=1, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |  \u001b[92m\u001b[1m     2.3   \u001b[0m\u001b[0m  |    6.3\n",
            "      [b=1, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    6.6\n",
            "      [b=1, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[2m\u001b[91m     6.8   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 20.9\u001b[0m\u001b[0m\n",
            "      [b=1, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m 20.6\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       2.6     |    6.5\n",
            "      [b=4, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    7.0\n",
            "      [b=4, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[2m\u001b[91m     5.5   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 12.2\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m 12.1\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       2.9     |    8.3\n",
            "      [b=4, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    8.2\n",
            "      [b=4, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[2m\u001b[91m    10.5   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 29.1\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[31m\u001b[1m 28.8\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       3.0     |    8.1\n",
            "      [b=4, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    7.5\n",
            "      [b=4, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[2m\u001b[91m    10.7   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 25.8\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m 25.5\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       4.1     |   10.9\n",
            "      [b=4, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |   10.8\n",
            "      [b=4, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[31m\u001b[1m    20.7   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 46.6\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[31m\u001b[1m 46.1\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       2.9     |    7.9\n",
            "      [b=8, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    8.0\n",
            "      [b=8, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[2m\u001b[91m    10.0   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 23.9\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m 23.8\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       3.6     |    9.5\n",
            "      [b=8, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    9.5\n",
            "      [b=8, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[31m\u001b[1m    19.9   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 46.3\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[31m\u001b[1m 45.7\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       3.6     |   10.2\n",
            "      [b=8, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    9.4\n",
            "      [b=8, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[31m\u001b[1m    20.0   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 43.7\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[31m\u001b[1m 43.5\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       4.2     |  \u001b[2m\u001b[91m 16.3\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |  \u001b[2m\u001b[91m 16.0\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[31m\u001b[1m    38.7   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 94.9\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[31m\u001b[1m 94.0\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |       3.6     |    8.7\n",
            "      [b=16, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |    8.6\n",
            "      [b=16, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m    19.4   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 46.5\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[31m\u001b[1m 46.1\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |       4.0     |  \u001b[2m\u001b[91m 16.4\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |  \u001b[2m\u001b[91m 16.1\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m    38.0   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m101.9\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[31m\u001b[1m100.3\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |       3.9     |  \u001b[2m\u001b[91m 16.0\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |  \u001b[2m\u001b[91m 15.8\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m    38.4   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m106.0\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[31m\u001b[1m104.4\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[2m\u001b[91m     6.2   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 31.9\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |  \u001b[31m\u001b[1m 31.3\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |               |  \u001b[31m\u001b[1m213.5\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[31m\u001b[1m210.9\u001b[0m\u001b[0m\n",
            "\n",
            "Times are in milliseconds (ms).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Mode"
      ],
      "metadata": {
        "id": "E7_umfTVy51d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/REQUIRES_GRAD = True/REQUIRES_GRAD = False/g' script.py\n",
        "!python script.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EWrh_4Tyrap",
        "outputId": "cad71474-5fd7-4359-92fe-2fa1a858cfb0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch : 1.10.0+cu111\n",
            "Speechbrain: 0.5.11\n",
            "warprnnt_numba: 0.4.0\n",
            "Numba supports CUDA: True\n",
            "Tue Feb  1 08:37:21 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    27W /  70W |   1423MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            "GPU 0: Tesla T4 (UUID: GPU-813da1c7-e3b9-50ad-128d-14ead9928af8)\n",
            "\n",
            "GPU Memory : |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "Gradients will be computed : False\n",
            "Computing : [b=1, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Computing : [b=1, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=1, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (4) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Computing : [b=4, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=4, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (8) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Computing : [b=8, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=8, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (16) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Computing : [b=16, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=16, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=32, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (32) < 2 * SM count (80) will likely result in GPU under utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Computing : [b=32, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=32, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=32, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=32, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=32, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=32, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=32, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=32, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=32, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=32, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=32, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=32, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=32, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]\n",
            "Computing : [b=32, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]\n",
            "Computing : [b=32, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print out results\n",
        "\n",
        "Since the output has been written to a pickle file, print out the output of the script above."
      ],
      "metadata": {
        "id": "VDf12SvNzUBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print()\n",
        "\n",
        "results_path = os.path.join(basedir, 'rnnt_results.pkl')\n",
        "results = load_results(results_path)\n",
        "compare = benchmark.Compare(results)\n",
        "compare.colorize()\n",
        "compare.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oVfaIEgzGIZ",
        "outputId": "7d9b6bf7-0dac-41e5-a6e9-bfa7db05c62a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[---------------------------------------------- RNNTLoss ----------------------------------------------]\n",
            "                                                                                |  SpeechBrain  |  Numba\n",
            "1 threads: ---------------------------------------------------------------------------------------------\n",
            "      [b=1, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       2.8     |  \u001b[34m\u001b[1m  3.2\u001b[0m\u001b[0m\n",
            "      [b=1, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |  \u001b[34m\u001b[1m  3.2\u001b[0m\u001b[0m\n",
            "      [b=1, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[34m\u001b[1m     2.4   \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  3.0\u001b[0m\u001b[0m\n",
            "      [b=1, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[34m\u001b[1m  3.1\u001b[0m\u001b[0m\n",
            "      [b=1, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |  \u001b[92m\u001b[1m     2.3   \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m  3.3\u001b[0m\u001b[0m\n",
            "      [b=1, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    3.5\n",
            "      [b=1, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |       3.6     |  \u001b[34m\u001b[1m  3.2\u001b[0m\u001b[0m\n",
            "      [b=1, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[34m\u001b[1m  3.1\u001b[0m\u001b[0m\n",
            "      [b=1, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |  \u001b[34m\u001b[1m     2.4   \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m  3.1\u001b[0m\u001b[0m\n",
            "      [b=1, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    3.5\n",
            "      [b=1, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |       3.8     |    3.5\n",
            "      [b=1, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |    3.6\n",
            "      [b=1, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       2.9     |    3.7\n",
            "      [b=1, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    3.9\n",
            "      [b=1, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[2m\u001b[91m     6.7   \u001b[0m\u001b[0m  |    5.1\n",
            "      [b=1, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |    5.1\n",
            "      [b=4, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       2.8     |  \u001b[34m\u001b[1m  3.1\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    3.4\n",
            "      [b=4, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[2m\u001b[91m     5.5   \u001b[0m\u001b[0m  |    4.4\n",
            "      [b=4, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |    4.5\n",
            "      [b=4, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       2.8     |    3.4\n",
            "      [b=4, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |  \u001b[34m\u001b[1m  3.2\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[2m\u001b[91m    10.5   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  7.7\u001b[0m\u001b[0m\n",
            "      [b=4, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m  7.7\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       2.6     |    3.6\n",
            "      [b=4, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |  \u001b[34m\u001b[1m  3.3\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[2m\u001b[91m    10.7   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  7.8\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m  7.8\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       4.0     |    4.2\n",
            "      [b=4, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    4.2\n",
            "      [b=4, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[31m\u001b[1m    20.6   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 14.4\u001b[0m\u001b[0m\n",
            "      [b=4, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m 14.4\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |  \u001b[34m\u001b[1m     2.5   \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m  3.1\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    3.4\n",
            "      [b=8, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[2m\u001b[91m    10.0   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  7.6\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m  7.6\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       3.5     |    4.0\n",
            "      [b=8, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    4.0\n",
            "      [b=8, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[31m\u001b[1m    19.9   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 14.1\u001b[0m\u001b[0m\n",
            "      [b=8, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m 14.1\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       3.6     |    4.0\n",
            "      [b=8, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |    4.1\n",
            "      [b=8, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[31m\u001b[1m    20.0   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 14.4\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[2m\u001b[91m 14.2\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]       |       4.1     |  \u001b[2m\u001b[91m  6.6\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]     |               |  \u001b[2m\u001b[91m  6.6\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]     |  \u001b[31m\u001b[1m    38.7   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 27.8\u001b[0m\u001b[0m\n",
            "      [b=8, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]   |               |  \u001b[31m\u001b[1m 27.9\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |       3.4     |    3.8\n",
            "      [b=16, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |    3.7\n",
            "      [b=16, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m    19.4   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 13.9\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[2m\u001b[91m 14.1\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |       4.1     |  \u001b[2m\u001b[91m  6.3\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |  \u001b[2m\u001b[91m  6.3\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m    38.0   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 28.3\u001b[0m\u001b[0m\n",
            "      [b=16, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[31m\u001b[1m 28.4\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |       3.9     |  \u001b[2m\u001b[91m  6.3\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |  \u001b[2m\u001b[91m  6.4\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m    38.5   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 28.6\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[31m\u001b[1m 28.6\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[2m\u001b[91m     6.2   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 11.6\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |  \u001b[2m\u001b[91m 11.6\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |               |  \u001b[31m\u001b[1m 59.8\u001b[0m\u001b[0m\n",
            "      [b=16, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[31m\u001b[1m 59.1\u001b[0m\u001b[0m\n",
            "      [b=32, t=200, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |       3.8     |  \u001b[2m\u001b[91m  6.1\u001b[0m\u001b[0m\n",
            "      [b=32, t=200, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |  \u001b[2m\u001b[91m  6.2\u001b[0m\u001b[0m\n",
            "      [b=32, t=200, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |  \u001b[31m\u001b[1m    37.4   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 28.8\u001b[0m\u001b[0m\n",
            "      [b=32, t=200, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[31m\u001b[1m 29.3\u001b[0m\u001b[0m\n",
            "      [b=32, t=200, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[2m\u001b[91m     5.1   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 11.3\u001b[0m\u001b[0m\n",
            "      [b=32, t=200, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |  \u001b[2m\u001b[91m 11.3\u001b[0m\u001b[0m\n",
            "      [b=32, t=200, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |               |  \u001b[31m\u001b[1m 59.9\u001b[0m\u001b[0m\n",
            "      [b=32, t=200, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[31m\u001b[1m 60.0\u001b[0m\u001b[0m\n",
            "      [b=32, t=400, u=100, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[2m\u001b[91m     5.3   \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 11.5\u001b[0m\u001b[0m\n",
            "      [b=32, t=400, u=100, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |  \u001b[2m\u001b[91m 11.4\u001b[0m\u001b[0m\n",
            "      [b=32, t=400, u=100, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |               |  \u001b[31m\u001b[1m 60.9\u001b[0m\u001b[0m\n",
            "      [b=32, t=400, u=100, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[31m\u001b[1m 60.1\u001b[0m\u001b[0m\n",
            "      [b=32, t=400, u=200, v=28, fastemit_lambda=0.0, dtype=torch.float32]      |  \u001b[2m\u001b[91m    10.4   \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 21.8\u001b[0m\u001b[0m\n",
            "      [b=32, t=400, u=200, v=28, fastemit_lambda=0.001, dtype=torch.float32]    |               |  \u001b[31m\u001b[1m 21.9\u001b[0m\u001b[0m\n",
            "      [b=32, t=400, u=200, v=1024, fastemit_lambda=0.0, dtype=torch.float32]    |               |  \u001b[31m\u001b[1m115.5\u001b[0m\u001b[0m\n",
            "      [b=32, t=400, u=200, v=1024, fastemit_lambda=0.001, dtype=torch.float32]  |               |  \u001b[31m\u001b[1m115.9\u001b[0m\u001b[0m\n",
            "\n",
            "Times are in milliseconds (ms).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nlJILyQBUOza"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}